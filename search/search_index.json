{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EESSI project documentation! \u00b6 Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Software testing Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording: More recently (Nov 25th 2020), EESSI was also the topic of a presentation at the International Series of Online Research Software Events (SORSE) , see:","title":"Home"},{"location":"#welcome-to-the-eessi-project-documentation","text":"Quote What if there was a way to avoid having to install a broad range of scientific software from scratch on every HPC cluster or cloud instance you use or maintain, without compromising on performance? The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in HPC community. The goal of this project is to build a common stack of scientific software installations for HPC systems and beyond, including laptops, personal workstations and cloud infrastructure. More details about the project are available in the different subsections: Project overview Filesystem layer Compatibility layer Software layer Pilot repository Software testing Project partners Contact info The EESSI project was presented at the HPC Knowledge Meeting in June 2020. Check the recording: More recently (Nov 25th 2020), EESSI was also the topic of a presentation at the International Series of Online Research Software Events (SORSE) , see:","title":"Welcome to the EESSI project documentation!"},{"location":"compatibility_layer/","text":"Compatibility layer \u00b6 The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"compatibility_layer/#compatibility-layer","text":"The middle layer of the EESSI project is the compatibility layer , which ensures that our scientific software stack is compatible with different client operating systems (different Linux distributions, macOS and even Windows via WSL ). For this we rely on Gentoo Prefix , by installing a limited set of Gentoo Linux packages in a non-standard location (a \"prefix\"), using Gentoo's package manager Portage . The compatible layer is maintained via our https://github.com/EESSI/compatibility-layer GitHub repository.","title":"Compatibility layer"},{"location":"contact/","text":"Contact info \u00b6 For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"contact/#contact-info","text":"For more information: visit our website https://www.eessi-hpc.org consult our documentation at https://eessi.github.io reach out to one of the project partners check out our GitHub repositories at https://github.com/EESSI follow us on Twitter: https://twitter.com/eessi_hpc A Slack channel is available for the EESSI community (an invitation is required to join).","title":"Contact info"},{"location":"filesystem_layer/","text":"Filesystem layer \u00b6 The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Overview"},{"location":"filesystem_layer/#filesystem-layer","text":"The bottom layer of the EESSI project is the filesystem layer , which is responsible for distributing the software stack. For this we rely on CernVM-FS (or CVMFS for short), a network file system used to distribute the software to the clients in a fast, reliable and scalable way. CVMFS was created over 10 years ago specifically for the purpose of globally distributing a large software stack. For the experiments at the Large Hadron Collider, it hosts several hundred million files and directories that are distributed to the order of hundred thousand client computers. The hierarchical structure with multiple caching layers (Stratum-0, Stratum-1's located at partner sites and local caching proxies) ensures good performance with limited resources. Redundancy is provided by using multiple Stratum-1's at various sites. Since CVMFS is based on the HTTP protocol, the ubiquitous Squid caching proxy can be leveraged to reduce server loads and improve performance at large installations (such as HPC clusters). Clients can easily mount the file system (read-only) via a FUSE (Filesystem in Userspace) module. For a (basic) introduction to CernVM-FS, see this presentation . Detailed information about how we configure CVMFS is available at https://github.com/EESSI/filesystem-layer .","title":"Filesystem layer"},{"location":"overview/","text":"Overview of the EESSI project \u00b6 Scope & Goals \u00b6 Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently. Inspiration \u00b6 The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented . Layered structure \u00b6 The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on. Opportunities \u00b6 We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community. Challenges \u00b6 There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI; Current status \u00b6 (June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Project overview"},{"location":"overview/#overview-of-the-eessi-project","text":"","title":"Overview of the EESSI project"},{"location":"overview/#scope-goals","text":"Through the EESSI project, we want to set up a shared stack of scientific software installations , and by doing so avoid a lot of duplicate work across HPC sites. For end users, we want to provide a uniform user experience with respect to available scientific software, regardless of which system they use. Our software stack should work on laptops, personal workstations, HPC clusters and in the cloud, which means we will need to support different CPUs, networks, GPUs, and so on. We hope to make this work for any Linux distribution and maybe even macOS and Windows via WSL , and a wide variety of CPU architectures (Intel, AMD, ARM, POWER, RISC-V). Of course we want to focus on the performance of the software, but also on automating the workflow for maintaining the software stack, thoroughly testing the installations, and collaborating efficiently.","title":"Scope &amp; Goals"},{"location":"overview/#inspiration","text":"The EESSI concept is heavily inspired by Compute Canada software stack, which is a shared software stack used on all 5 major national systems in Canada and a bunch of smaller ones. The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\" . It has also been presented at the 5th EasyBuild User Meetings ( slides , recorded talk ), and is well documented .","title":"Inspiration"},{"location":"overview/#layered-structure","text":"The EESSI project consists of 3 layers. The bottom layer is the filesystem layer , which is responsible for distributing the software stack across clients. The middle layer is a compatibility layer , which ensures that the software stack is compatible with multiple different client operating systems. The top layer is the software layer , which contains the actual scientific software applications and their dependencies. The host OS still provides a couple of things, like drivers for network and GPU, support for shared filesystems like GPFS and Lustre, a resource manager like Slurm, and so on.","title":"Layered structure"},{"location":"overview/#opportunities","text":"We hope to collaborate with interested parties across the HPC community, including HPC centres, vendors, consultancy companies and scientific software developers. Through our software stack, HPC users can seamlessly hop between sites, since the same software is available everywhere. We can leverage each others work with respect to providing tested and properly optimized scientific software installations more efficiently, and provide a platform for easy benchmarking of new systems. By working together with the developers of scientific software we can provide vetted installations for the broad HPC community.","title":"Opportunities"},{"location":"overview/#challenges","text":"There are many challenges in an ambitious project like this, including (but probably not limited to): Finding time and manpower to get the software stack set up properly; Leveraging system sources like network interconnect (MPI & co), accelerators (GPUs), ...; Supporting CPU architectures other than x86_64, including ARM, POWER, RISC-V, ... Dealing with licensed software, like Intel tools, MATLAB, ANSYS, ...; Integration with resource managers (Slurm) and vendor provided software (Cray PE); Convincing HPC site admins to adopt EESSI;","title":"Challenges"},{"location":"overview/#current-status","text":"(June 2020) We are actively working on a pilot setup that has a limited scope, and are organizing monthly meetings to discuss progress and next steps forward. Keep an eye on our GitHub repositories at https://github.com/EESSI and our Twitter feed .","title":"Current status"},{"location":"partners/","text":"Project partners \u00b6 Delft University of Technology (The Netherlands) \u00b6 Robbert Eggermont Koen Mulderij Dell Technologies (Europe) \u00b6 Walther Blom, High Education & Research Jaco van Dijk, Higher Education Eindhoven University of Technology \u00b6 Patrick Van Brakel Ghent University (Belgium) \u00b6 Kenneth Hoste, HPC-UGent HPCNow! (Spain) \u00b6 Oriol Mula Valls J\u00fclich Supercomputing Centre (Germany) \u00b6 Alan O'Cais University of Cambridge (United Kingdom) \u00b6 Mark Sharpley, Research Computing Services Division University of Groningen (The Netherlands) \u00b6 Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology University of Twente (The Netherlands) \u00b6 Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS) University of Oslo (Norway) \u00b6 Terje Kvernes University of Bergen (Norway) \u00b6 Thomas R\u00f6blitz Vrije Universiteit Amsterdam (The Netherlands) \u00b6 Peter Stol SURF (The Netherlands) \u00b6 Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"Project partners"},{"location":"partners/#project-partners","text":"","title":"Project partners"},{"location":"partners/#delft-university-of-technology-the-netherlands","text":"Robbert Eggermont Koen Mulderij","title":"Delft University of Technology (The Netherlands)"},{"location":"partners/#dell-technologies-europe","text":"Walther Blom, High Education & Research Jaco van Dijk, Higher Education","title":"Dell Technologies (Europe)"},{"location":"partners/#eindhoven-university-of-technology","text":"Patrick Van Brakel","title":"Eindhoven University of Technology"},{"location":"partners/#ghent-university-belgium","text":"Kenneth Hoste, HPC-UGent","title":"Ghent University (Belgium)"},{"location":"partners/#hpcnow-spain","text":"Oriol Mula Valls","title":"HPCNow! (Spain)"},{"location":"partners/#julich-supercomputing-centre-germany","text":"Alan O'Cais","title":"J\u00fclich Supercomputing Centre (Germany)"},{"location":"partners/#university-of-cambridge-united-kingdom","text":"Mark Sharpley, Research Computing Services Division","title":"University of Cambridge (United Kingdom)"},{"location":"partners/#university-of-groningen-the-netherlands","text":"Bob Dr\u00f6ge, Center for Information Technology Henk-Jan Zilverberg, Center for Information Technology","title":"University of Groningen (The Netherlands)"},{"location":"partners/#university-of-twente-the-netherlands","text":"Geert Jan Laanstra, Electrical Engineering, Mathematics and Computer Science (EEMCS)","title":"University of Twente (The Netherlands)"},{"location":"partners/#university-of-oslo-norway","text":"Terje Kvernes","title":"University of Oslo (Norway)"},{"location":"partners/#university-of-bergen-norway","text":"Thomas R\u00f6blitz","title":"University of Bergen (Norway)"},{"location":"partners/#vrije-universiteit-amsterdam-the-netherlands","text":"Peter Stol","title":"Vrije Universiteit Amsterdam (The Netherlands)"},{"location":"partners/#surf-the-netherlands","text":"Caspar van Leeuwen Marco Verdicchio Bas van der Vlies","title":"SURF (The Netherlands)"},{"location":"pilot/","text":"Pilot software stack (2020.12) \u00b6 Caveats \u00b6 The current EESSI pilot software stack (version 2020.12) is the 4th iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems! Reporting problems \u00b6 If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues. Accessing the EESSI pilot repository through Singularity \u00b6 The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in Docker Hub (see https://hub.docker.com/r/eessi/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS. The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI config and pilot repositories (using the cvmfs2 command that is included in the container image): export EESSI_CONFIG = \"container:cvmfs2 cvmfs-config.eessi-hpc.org /cvmfs/cvmfs-config.eessi-hpc.org\" export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_CONFIG \" --fusemount \" $EESSI_PILOT \" docker://eessi/client-pilot:centos7- $( uname -m ) This should give you a shell in the container, where the EESSI config and pilot repositories are mounted: $ singularity shell --fusemount \"$EESSI_CONFIG\" --fusemount \"$EESSI_PILOT\" docker://eessi/client-pilot:centos7-$(uname -m) INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/2020.12 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/2020.12 compat init software Standard installation \u00b6 For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository . Here we prefer the Singularity approach as we can guarantee that the docker image is up to date. Setting up the EESSI environment \u00b6 Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/2020.12/init/bash If all goes well, you should see output like this: Singularity> source /cvmfs/pilot.eessi-hpc.org/2020.12/init/bash Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/2020.12! Using x86_64/intel/haswell as software subdirectory. Using /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH. Found Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/.lmod/lmodrc.lua Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2020 .12 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations! Testing the EESSI pilot software stack \u00b6 Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests . Giving feedback or reporting problems \u00b6 Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST) Available software \u00b6 (last update: Jan 7th 2021) EESSI currently supports the following HPC applications as well as all their dependencies: GROMACS (2020.1) OpenFOAM (v2006 and 8) R 4.0.0 + R-bundle-Bioconductor 3.11 TensorFlow (2.3.1) (currently not available on ppc64le ) OSU-Micro-Benchmarks (5.6.3) [EESSI pilot 2020.12] $ module --nx avail ------------------------------ /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2/modules/all ------------------------------ Bazel/3.6.0-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a Bison/3.5.3-GCCcore-9.3.0 SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 Boost/1.72.0-gompi-2020a Szip/2.1.1-GCCcore-9.3.0 CGAL/4.14.3-gompi-2020a-Python-3.8.2 Tcl/8.6.10-GCCcore-9.3.0 CMake/3.16.4-GCCcore-9.3.0 TensorFlow/2.3.1-foss-2020a-Python-3.8.2 DB/18.1.32-GCCcore-9.3.0 Tk/8.6.10-GCCcore-9.3.0 DBus/1.13.12-GCCcore-9.3.0 UCX/1.8.0-GCCcore-9.3.0 Doxygen/1.8.17-GCCcore-9.3.0 UDUNITS/2.2.26-foss-2020a EasyBuild/4.3.2 UnZip/6.0-GCCcore-9.3.0 Eigen/3.3.7-GCCcore-9.3.0 X11/20200222-GCCcore-9.3.0 FFTW/3.3.8-gompi-2020a Xvfb/1.20.9-GCCcore-9.3.0 FFmpeg/4.2.2-GCCcore-9.3.0 Zip/3.0-GCCcore-9.3.0 FriBidi/1.0.9-GCCcore-9.3.0 cairo/1.16.0-GCCcore-9.3.0 GCC/9.3.0 double-conversion/3.1.5-GCCcore-9.3.0 GCCcore/9.3.0 expat/2.2.9-GCCcore-9.3.0 GLPK/4.65-GCCcore-9.3.0 flatbuffers/1.12.0-GCCcore-9.3.0 GLib/2.64.1-GCCcore-9.3.0 fontconfig/2.13.92-GCCcore-9.3.0 GMP/6.2.0-GCCcore-9.3.0 foss/2020a GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2 freetype/2.10.1-GCCcore-9.3.0 GROMACS/2020.1-foss-2020a-Python-3.8.2 giflib/5.2.1-GCCcore-9.3.0 GSL/2.6-GCC-9.3.0 git/2.23.0-GCCcore-9.3.0-nodocs Ghostscript/9.52-GCCcore-9.3.0 gnuplot/5.2.8-GCCcore-9.3.0 HDF5/1.10.6-gompi-2020a gompi/2020a HarfBuzz/2.6.4-GCCcore-9.3.0 groff/1.22.4-GCCcore-9.3.0 ICU/66.1-GCCcore-9.3.0 gzip/1.10-GCCcore-9.3.0 ImageMagick/7.0.10-1-GCCcore-9.3.0 h5py/2.10.0-foss-2020a-Python-3.8.2 JasPer/2.0.14-GCCcore-9.3.0 hwloc/2.2.0-GCCcore-9.3.0 Java/11.0.2 (11) libGLU/9.0.1-GCCcore-9.3.0 JsonCpp/1.9.4-GCCcore-9.3.0 libcerf/1.13-GCCcore-9.3.0 LAME/3.100-GCCcore-9.3.0 libdrm/2.4.100-GCCcore-9.3.0 LLVM/9.0.1-GCCcore-9.3.0 libevent/2.1.11-GCCcore-9.3.0 LMDB/0.9.24-GCCcore-9.3.0 libfabric/1.11.0-GCCcore-9.3.0 LibTIFF/4.1.0-GCCcore-9.3.0 libffi/3.3-GCCcore-9.3.0 LittleCMS/2.9-GCCcore-9.3.0 libgd/2.3.0-GCCcore-9.3.0 Lua/5.3.5-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 METIS/5.1.0-GCCcore-9.3.0 libiconv/1.16-GCCcore-9.3.0 MPFR/4.0.2-GCCcore-9.3.0 libjpeg-turbo/2.0.4-GCCcore-9.3.0 Mako/1.1.2-GCCcore-9.3.0 libpciaccess/0.16-GCCcore-9.3.0 MariaDB-connector-c/3.1.7-GCCcore-9.3.0 libpng/1.6.37-GCCcore-9.3.0 Mesa/20.0.2-GCCcore-9.3.0 libsndfile/1.0.28-GCCcore-9.3.0 Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 libunwind/1.3.1-GCCcore-9.3.0 NASM/2.14.02-GCCcore-9.3.0 libxml2/2.9.10-GCCcore-9.3.0 NLopt/2.6.1-GCCcore-9.3.0 lz4/1.9.2-GCCcore-9.3.0 NSPR/4.25-GCCcore-9.3.0 makeinfo/6.7-GCCcore-9.3.0 NSS/3.51-GCCcore-9.3.0 ncdf4/1.17-foss-2020a-R-4.0.0 Ninja/1.10.0-GCCcore-9.3.0 netCDF/4.7.4-gompi-2020a OSU-Micro-Benchmarks/5.6.3-gompi-2020a nettle/3.6-GCCcore-9.3.0 OpenBLAS/0.3.9-GCC-9.3.0 networkx/2.4-foss-2020a-Python-3.8.2 OpenFOAM/v2006-foss-2020a nsync/1.24.0-GCCcore-9.3.0 OpenFOAM/8-foss-2020a (D) numactl/2.0.13-GCCcore-9.3.0 OpenMPI/4.0.3-GCC-9.3.0 pixman/0.38.4-GCCcore-9.3.0 PCRE/8.44-GCCcore-9.3.0 pkg-config/0.29.2-GCCcore-9.3.0 PCRE2/10.34-GCCcore-9.3.0 pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2 PMIx/3.1.5-GCCcore-9.3.0 protobuf-python/3.13.0-foss-2020a-Python-3.8.2 Pango/1.44.7-GCCcore-9.3.0 protobuf/3.13.0-GCCcore-9.3.0 ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 Perl/5.30.2-GCCcore-9.3.0 re2c/1.3-GCCcore-9.3.0 Python/2.7.18-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 Python/3.8.2-GCCcore-9.3.0 (D) snappy/1.1.8-GCCcore-9.3.0 Qt5/5.14.1-GCCcore-9.3.0 util-linux/2.35-GCCcore-9.3.0 R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0 x264/20191217-GCCcore-9.3.0 R/4.0.0-foss-2020a x265/3.3-GCCcore-9.3.0 SCOTCH/6.0.9-gompi-2020a xorg-macros/1.19.2-GCCcore-9.3.0 SQLite/3.31.1-GCCcore-9.3.0 zstd/1.4.4-GCCcore-9.3.0 SWIG/4.0.1-GCCcore-9.3.0 Architecture and micro-architecture support \u00b6 x86_64 \u00b6 generic (currently implies march=x86-64 and -mtune=generic ) AMD zen2 (Rome) Intel haswell skylake_avx512 aarch64/arm64 \u00b6 generic (currently implies -march=armv8-a and -mtune=generic ) AWS Graviton2 Fujitsu A64FX Marvell Thunder X2 ppc64le \u00b6 IBM POWER9 EasyBuild configuration \u00b6 EasyBuild v4.3.2 was used to install the software in the 2020.12 version of the pilot repository. For some installations pull requests with changes that will be included in EasyBuild v4.3.2 were leveraged, see the build script that was used. An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/build containerpath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/containers debug (E) = True filter-deps (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, flex, gettext, gperf, help2man, intltool, libreadline, libtool, M4, ncurses, XZ, zlib filter-env-vars (E) = LD_LIBRARY_PATH ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2 module-extensions (E) = True repositorypath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/ebfiles_repo robot-paths (D) = /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2/software/EasyBuild/4.3.2/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/eessi-build/easybuild/sources: sysroot (E) = /cvmfs/pilot.eessi-hpc.org/2020.12/compat/linux/x86_64 trace (E) = True zip-logs (E) = bzip2","title":"Pilot repository"},{"location":"pilot/#pilot-software-stack-202012","text":"","title":"Pilot software stack (2020.12)"},{"location":"pilot/#caveats","text":"The current EESSI pilot software stack (version 2020.12) is the 4th iteration, and there are some known issues and limitations, please take these into account: First of all: the EESSI pilot software stack is NOT READY FOR PRODUCTION! Do not use it for production work, and be careful when testing it on production systems!","title":"Caveats"},{"location":"pilot/#reporting-problems","text":"If you notice any problems, please report them via https://github.com/EESSI/software-layer/issues.","title":"Reporting problems"},{"location":"pilot/#accessing-the-eessi-pilot-repository-through-singularity","text":"The easiest way to access the EESSI pilot repository is by using Singularity. If Singularity is installed already, no admin privileges are required. No other software is needed either on the host. A container image is available in Docker Hub (see https://hub.docker.com/r/eessi/client-pilot ). It only contains a minimal operating system + the necessary packages to access the EESSI pilot repository through CernVM-FS. The container image can be used directly by Singularity (no prior download required), as follows: First, create some local directories in /tmp/$USER which will be bind mounted in the container: mkdir -p /tmp/ $USER / { var-lib-cvmfs,var-run-cvmfs,home } These provides space for the CernVM-FS cache, and an empty home directory to use in the container. Set the $SINGULARITY_BIND and $SINGULARITY_HOME environment variables to configure Singularity: export SINGULARITY_BIND = \"/tmp/ $USER /var-run-cvmfs:/var/run/cvmfs,/tmp/ $USER /var-lib-cvmfs:/var/lib/cvmfs\" export SINGULARITY_HOME = \"/tmp/ $USER /home:/home/ $USER \" Start the container using singularity shell , using --fusemount to mount the EESSI config and pilot repositories (using the cvmfs2 command that is included in the container image): export EESSI_CONFIG = \"container:cvmfs2 cvmfs-config.eessi-hpc.org /cvmfs/cvmfs-config.eessi-hpc.org\" export EESSI_PILOT = \"container:cvmfs2 pilot.eessi-hpc.org /cvmfs/pilot.eessi-hpc.org\" singularity shell --fusemount \" $EESSI_CONFIG \" --fusemount \" $EESSI_PILOT \" docker://eessi/client-pilot:centos7- $( uname -m ) This should give you a shell in the container, where the EESSI config and pilot repositories are mounted: $ singularity shell --fusemount \"$EESSI_CONFIG\" --fusemount \"$EESSI_PILOT\" docker://eessi/client-pilot:centos7-$(uname -m) INFO: Using cached SIF image CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: pre-mounted on file descriptor 3 CernVM-FS: loading Fuse module... done CernVM-FS: loading Fuse module... done Singularity> It is possible that you see some scary looking warnings, but those can be ignored for now. To verify that things are working, check the contents of the /cvmfs/pilot.eessi-hpc.org/2020.12 directory: Singularity> ls /cvmfs/pilot.eessi-hpc.org/2020.12 compat init software","title":"Accessing the EESSI pilot repository through Singularity"},{"location":"pilot/#standard-installation","text":"For those with privileges on their system, there are a number of example installation scripts for different architectures and operating systems available in the EESSI demo repository . Here we prefer the Singularity approach as we can guarantee that the docker image is up to date.","title":"Standard installation"},{"location":"pilot/#setting-up-the-eessi-environment","text":"Once you have the EESSI pilot repository mounted, you can set up the environment by sourcing the provided init script: source /cvmfs/pilot.eessi-hpc.org/2020.12/init/bash If all goes well, you should see output like this: Singularity> source /cvmfs/pilot.eessi-hpc.org/2020.12/init/bash Found EESSI pilot repo @ /cvmfs/pilot.eessi-hpc.org/2020.12! Using x86_64/intel/haswell as software subdirectory. Using /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/modules/all as the directory to be added to MODULEPATH. Found Lmod configuration file at /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/.lmod/lmodrc.lua Initializing Lmod... Prepending /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/intel/haswell/modules/all to $MODULEPATH ... Environment set up to use EESSI pilot software stack, have fun! [ EESSI pilot 2020 .12 ] $ Now you're all set up! Go ahead and explore the software stack using \" module avail \", and go wild with testing the available software installations!","title":"Setting up the EESSI environment"},{"location":"pilot/#testing-the-eessi-pilot-software-stack","text":"Please test the EESSI pilot software stack as you see fit: running simple commands, performing small calculations or running small benchmarks, etc. Test scripts that have been verified to work correctly using the pilot software stack are available at https://github.com/EESSI/software-layer/tree/main/tests .","title":"Testing the EESSI pilot software stack"},{"location":"pilot/#giving-feedback-or-reporting-problems","text":"Any feedback is welcome, and questions or problems reports are welcome as well, through one of the EESSI communication channels: ( preferred! ) EESSI software-layer GitHub repository: https://github.com/EESSI/software-layer/issues EESSI mailing list ( eessi@list.rug.nl ) EESSI Slack: https://eessi-hpc.slack.com (get an invite via https://www.eessi-hpc.org/join ) monthly EESSI meetings (first Thursday of the month at 2pm CEST)","title":"Giving feedback or reporting problems"},{"location":"pilot/#available-software","text":"(last update: Jan 7th 2021) EESSI currently supports the following HPC applications as well as all their dependencies: GROMACS (2020.1) OpenFOAM (v2006 and 8) R 4.0.0 + R-bundle-Bioconductor 3.11 TensorFlow (2.3.1) (currently not available on ppc64le ) OSU-Micro-Benchmarks (5.6.3) [EESSI pilot 2020.12] $ module --nx avail ------------------------------ /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2/modules/all ------------------------------ Bazel/3.6.0-GCCcore-9.3.0 ScaLAPACK/2.1.0-gompi-2020a Bison/3.5.3-GCCcore-9.3.0 SciPy-bundle/2020.03-foss-2020a-Python-3.8.2 Boost/1.72.0-gompi-2020a Szip/2.1.1-GCCcore-9.3.0 CGAL/4.14.3-gompi-2020a-Python-3.8.2 Tcl/8.6.10-GCCcore-9.3.0 CMake/3.16.4-GCCcore-9.3.0 TensorFlow/2.3.1-foss-2020a-Python-3.8.2 DB/18.1.32-GCCcore-9.3.0 Tk/8.6.10-GCCcore-9.3.0 DBus/1.13.12-GCCcore-9.3.0 UCX/1.8.0-GCCcore-9.3.0 Doxygen/1.8.17-GCCcore-9.3.0 UDUNITS/2.2.26-foss-2020a EasyBuild/4.3.2 UnZip/6.0-GCCcore-9.3.0 Eigen/3.3.7-GCCcore-9.3.0 X11/20200222-GCCcore-9.3.0 FFTW/3.3.8-gompi-2020a Xvfb/1.20.9-GCCcore-9.3.0 FFmpeg/4.2.2-GCCcore-9.3.0 Zip/3.0-GCCcore-9.3.0 FriBidi/1.0.9-GCCcore-9.3.0 cairo/1.16.0-GCCcore-9.3.0 GCC/9.3.0 double-conversion/3.1.5-GCCcore-9.3.0 GCCcore/9.3.0 expat/2.2.9-GCCcore-9.3.0 GLPK/4.65-GCCcore-9.3.0 flatbuffers/1.12.0-GCCcore-9.3.0 GLib/2.64.1-GCCcore-9.3.0 fontconfig/2.13.92-GCCcore-9.3.0 GMP/6.2.0-GCCcore-9.3.0 foss/2020a GObject-Introspection/1.64.0-GCCcore-9.3.0-Python-3.8.2 freetype/2.10.1-GCCcore-9.3.0 GROMACS/2020.1-foss-2020a-Python-3.8.2 giflib/5.2.1-GCCcore-9.3.0 GSL/2.6-GCC-9.3.0 git/2.23.0-GCCcore-9.3.0-nodocs Ghostscript/9.52-GCCcore-9.3.0 gnuplot/5.2.8-GCCcore-9.3.0 HDF5/1.10.6-gompi-2020a gompi/2020a HarfBuzz/2.6.4-GCCcore-9.3.0 groff/1.22.4-GCCcore-9.3.0 ICU/66.1-GCCcore-9.3.0 gzip/1.10-GCCcore-9.3.0 ImageMagick/7.0.10-1-GCCcore-9.3.0 h5py/2.10.0-foss-2020a-Python-3.8.2 JasPer/2.0.14-GCCcore-9.3.0 hwloc/2.2.0-GCCcore-9.3.0 Java/11.0.2 (11) libGLU/9.0.1-GCCcore-9.3.0 JsonCpp/1.9.4-GCCcore-9.3.0 libcerf/1.13-GCCcore-9.3.0 LAME/3.100-GCCcore-9.3.0 libdrm/2.4.100-GCCcore-9.3.0 LLVM/9.0.1-GCCcore-9.3.0 libevent/2.1.11-GCCcore-9.3.0 LMDB/0.9.24-GCCcore-9.3.0 libfabric/1.11.0-GCCcore-9.3.0 LibTIFF/4.1.0-GCCcore-9.3.0 libffi/3.3-GCCcore-9.3.0 LittleCMS/2.9-GCCcore-9.3.0 libgd/2.3.0-GCCcore-9.3.0 Lua/5.3.5-GCCcore-9.3.0 libglvnd/1.2.0-GCCcore-9.3.0 METIS/5.1.0-GCCcore-9.3.0 libiconv/1.16-GCCcore-9.3.0 MPFR/4.0.2-GCCcore-9.3.0 libjpeg-turbo/2.0.4-GCCcore-9.3.0 Mako/1.1.2-GCCcore-9.3.0 libpciaccess/0.16-GCCcore-9.3.0 MariaDB-connector-c/3.1.7-GCCcore-9.3.0 libpng/1.6.37-GCCcore-9.3.0 Mesa/20.0.2-GCCcore-9.3.0 libsndfile/1.0.28-GCCcore-9.3.0 Meson/0.55.1-GCCcore-9.3.0-Python-3.8.2 libunwind/1.3.1-GCCcore-9.3.0 NASM/2.14.02-GCCcore-9.3.0 libxml2/2.9.10-GCCcore-9.3.0 NLopt/2.6.1-GCCcore-9.3.0 lz4/1.9.2-GCCcore-9.3.0 NSPR/4.25-GCCcore-9.3.0 makeinfo/6.7-GCCcore-9.3.0 NSS/3.51-GCCcore-9.3.0 ncdf4/1.17-foss-2020a-R-4.0.0 Ninja/1.10.0-GCCcore-9.3.0 netCDF/4.7.4-gompi-2020a OSU-Micro-Benchmarks/5.6.3-gompi-2020a nettle/3.6-GCCcore-9.3.0 OpenBLAS/0.3.9-GCC-9.3.0 networkx/2.4-foss-2020a-Python-3.8.2 OpenFOAM/v2006-foss-2020a nsync/1.24.0-GCCcore-9.3.0 OpenFOAM/8-foss-2020a (D) numactl/2.0.13-GCCcore-9.3.0 OpenMPI/4.0.3-GCC-9.3.0 pixman/0.38.4-GCCcore-9.3.0 PCRE/8.44-GCCcore-9.3.0 pkg-config/0.29.2-GCCcore-9.3.0 PCRE2/10.34-GCCcore-9.3.0 pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2 PMIx/3.1.5-GCCcore-9.3.0 protobuf-python/3.13.0-foss-2020a-Python-3.8.2 Pango/1.44.7-GCCcore-9.3.0 protobuf/3.13.0-GCCcore-9.3.0 ParaView/5.8.0-foss-2020a-Python-3.8.2-mpi pybind11/2.4.3-GCCcore-9.3.0-Python-3.8.2 Perl/5.30.2-GCCcore-9.3.0 re2c/1.3-GCCcore-9.3.0 Python/2.7.18-GCCcore-9.3.0 scikit-build/0.10.0-foss-2020a-Python-3.8.2 Python/3.8.2-GCCcore-9.3.0 (D) snappy/1.1.8-GCCcore-9.3.0 Qt5/5.14.1-GCCcore-9.3.0 util-linux/2.35-GCCcore-9.3.0 R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0 x264/20191217-GCCcore-9.3.0 R/4.0.0-foss-2020a x265/3.3-GCCcore-9.3.0 SCOTCH/6.0.9-gompi-2020a xorg-macros/1.19.2-GCCcore-9.3.0 SQLite/3.31.1-GCCcore-9.3.0 zstd/1.4.4-GCCcore-9.3.0 SWIG/4.0.1-GCCcore-9.3.0","title":"Available software"},{"location":"pilot/#architecture-and-micro-architecture-support","text":"","title":"Architecture and micro-architecture support"},{"location":"pilot/#x86_64","text":"generic (currently implies march=x86-64 and -mtune=generic ) AMD zen2 (Rome) Intel haswell skylake_avx512","title":"x86_64"},{"location":"pilot/#aarch64arm64","text":"generic (currently implies -march=armv8-a and -mtune=generic ) AWS Graviton2 Fujitsu A64FX Marvell Thunder X2","title":"aarch64/arm64"},{"location":"pilot/#ppc64le","text":"IBM POWER9","title":"ppc64le"},{"location":"pilot/#easybuild-configuration","text":"EasyBuild v4.3.2 was used to install the software in the 2020.12 version of the pilot repository. For some installations pull requests with changes that will be included in EasyBuild v4.3.2 were leveraged, see the build script that was used. An example configuration of the build environment based on https://github.com/EESSI/software-layer can be seen here: $ eb --show-config # # Current EasyBuild configuration # (C: command line argument, D: default value, E: environment variable, F: configuration file) # buildpath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/build containerpath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/containers debug (E) = True filter-deps (E) = Autoconf, Automake, Autotools, binutils, bzip2, cURL, flex, gettext, gperf, help2man, intltool, libreadline, libtool, M4, ncurses, XZ, zlib filter-env-vars (E) = LD_LIBRARY_PATH ignore-osdeps (E) = True installpath (E) = /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2 module-extensions (E) = True repositorypath (D) = /tmp/eessi-build/singularity-home/.local/easybuild/ebfiles_repo robot-paths (D) = /cvmfs/pilot.eessi-hpc.org/2020.12/software/x86_64/amd/zen2/software/EasyBuild/4.3.2/easybuild/easyconfigs rpath (E) = True sourcepath (E) = /tmp/eessi-build/easybuild/sources: sysroot (E) = /cvmfs/pilot.eessi-hpc.org/2020.12/compat/linux/x86_64 trace (E) = True zip-logs (E) = bzip2","title":"EasyBuild configuration"},{"location":"software_layer/","text":"Software layer \u00b6 The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"},{"location":"software_layer/#software-layer","text":"The top layer of the EESSI project is the software layer , which provides the actual scientific software installations. To install the software we include in our stack, we use EasyBuild , a framework for installing scientific software on HPC systems. These installations are optimized for a particular system architecture (specific CPU and GPU generation). To access these software installation we provide environment module files and use Lmod , a modern environment modules tool which has been widely adopted in the HPC community in recent years. We leverage the archspec Python library to automatically select the best suited part of the software stack for a particular host, based on its system architecture. The software layer is maintained through our https://github.com/EESSI/software-layer GitHub repository.","title":"Software layer"},{"location":"software_testing/","text":"Software testing \u00b6 WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change. Description of the software test suite \u00b6 Framework \u00b6 The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs. Test variants \u00b6 The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc. The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack. By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags: container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses singularity exec to launch the EESSI container, load the GROMACS module, and run the GROMACS test. native : tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test. singlecore : tests designed to run on a single core singlenode : tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing) small : tests designed to run on 2-8 nodes. large : tests designed to run on >9 nodes. cpu : test designed to run on CPU. gpu , gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs. How to run the test suite \u00b6 General requirements \u00b6 A copy of the tests directory from software repository Requirements for container-based tests \u00b6 Specifically for container-based tests, there are some requirements on the host system: An installation of ReFrame An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support) Singularity The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The shared_alien_cache_minimal.sh script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the SHAREDSPACE and LOCALSPACE variables in that script for your system, and point them to a shared and node-local directory. Setting up a ReFrame configuration file \u00b6 Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation ). If you want to use the container-based tests, you have to define a partition programming environment called container and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the tests/reframe/config/settings.py in the software-layer repository . Other than (potential) adaptations to the container environment, you should only really need to change the systems part. Adapting the tests to your system \u00b6 For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all self.num_tasks_per_node you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node). An example run \u00b6 In this example, we assume your current directory is the tests/reframe folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu (assuming you adapted the config file in config/settings.py for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the -l argument into a -r : reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report To run the same tests with using the EESSI container, run: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.","title":"Software testing"},{"location":"software_testing/#software-testing","text":"WARNING: development of the software test suite has only just started and is a work in progress. This page describes how the test suite will be designed, but many things are not implemented yet and the design may still change.","title":"Software testing"},{"location":"software_testing/#description-of-the-software-test-suite","text":"","title":"Description of the software test suite"},{"location":"software_testing/#framework","text":"The EESSI project uses the ReFrame framework for software testing. ReFrame is designed particularly for testing HPC software and thus has well integrated support for interacting with schedulers, as well as various launchers for MPI programs.","title":"Framework"},{"location":"software_testing/#test-variants","text":"The EESSI software stack can be used in various ways, e.g. by using the container or when the CVMFS software stack is mounted natively. This means the commands that need to be run to test an application are different in both cases. Similarly, systems may have different hardware (CPUs v.s. GPUs, system size, etc). Thus, tests - e.g. a GROMACS test - may have different variants: one designed to run on CPUs, one on GPUs, one designed to run through the container, etc. The main goal of the EESSI test suite is to test the software stack on systems that have the EESSI CVMFS mounted natively. Some tests may also have variants that can run the same test through the container, but note that this setup is technically much more difficult. Thus, the main focus is on tests that run with a native CVMFS mount of the EESSI stack. By default, ReFrame runs all test variants it find. Thus, in our test suite, we prespecify a number of tags that can be used to select an appropriate subset of tests for your system. We recognize the following tags: container: tests that use the EESSI container to run the software. E.g. one variant of our GROMACS test uses singularity exec to launch the EESSI container, load the GROMACS module, and run the GROMACS test. native : tests that rely on the EESSI software stack being available through the modules system. E.g. one variant of the GROMACS test loads the GROMACS module and runs the GROMACS test. singlecore : tests designed to run on a single core singlenode : tests designed to run on a single (multicore) node (note: may still use MPI for multiprocessing) small : tests designed to run on 2-8 nodes. large : tests designed to run on >9 nodes. cpu : test designed to run on CPU. gpu , gpu_nvidia, gpu_amd: test designed to run on GPUs / nvidia GPUs / AMD GPUs.","title":"Test variants"},{"location":"software_testing/#how-to-run-the-test-suite","text":"","title":"How to run the test suite"},{"location":"software_testing/#general-requirements","text":"A copy of the tests directory from software repository","title":"General requirements"},{"location":"software_testing/#requirements-for-container-based-tests","text":"Specifically for container-based tests, there are some requirements on the host system: An installation of ReFrame An MPI installation (to launch MPI tests) or PMIx-based launcher (e.g. SLURM compiled with PMIx support) Singularity The container based tests will use a so-called shared alien CVMFS cache to store temporary data. In addition, they use a local CVMFS cache for speed. For this reason, the container tests need to be pointed to one directory that is shared between nodes on your system, and one directory that is node-specific (preferably a local disk). The shared_alien_cache_minimal.sh script that is part of the test suite defines these, and sets up the correct CVMFS configuration. You will have to adapt the SHAREDSPACE and LOCALSPACE variables in that script for your system, and point them to a shared and node-local directory.","title":"Requirements for container-based tests"},{"location":"software_testing/#setting-up-a-reframe-configuration-file","text":"Once the prerequisites have been met, you'll need to create a ReFrame configuration file that matches your system (see the ReFrame documentation ). If you want to use the container-based tests, you have to define a partition programming environment called container and make sure it loads any modules needed to provide the MPI installation and singularity command. For an example configuration file, check the tests/reframe/config/settings.py in the software-layer repository . Other than (potential) adaptations to the container environment, you should only really need to change the systems part.","title":"Setting up a ReFrame configuration file"},{"location":"software_testing/#adapting-the-tests-to-your-system","text":"For now, you will have to adapt the number of tasks specified in full-node tests to match the number of cores your machine has in a single node (in the future, you should be able to do this through the reframe configuration file). To do so, change all self.num_tasks_per_node you find in the various tests to that core count (unless they are 1, in which case the test specifically intended for only 1 process per node).","title":"Adapting the tests to your system"},{"location":"software_testing/#an-example-run","text":"In this example, we assume your current directory is the tests/reframe folder. To list e.g. all single node, cpu-based application tests on a system that has the EESSI software environment available natively, you execute: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu (assuming you adapted the config file in config/settings.py for your system). This should list the tests that are selected based on the provided tags. To run the tests, change the -l argument into a -r : reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t native -t single -t cpu --performance-report To run the same tests with using the EESSI container, run: reframe --config-file=config/settings.py --checkpath eessi-checks/applications/ -l -t container -t single -t cpu --performance-report Note that not all tests necessarily have implementations to run using the EESSI container: the primary focus of the test suite is for HPC sites to check the performance of their software suite. Such sites should have CVMFS mounted natively for optimal performance anyway.","title":"An example run"},{"location":"filesystem_layer/stratum1/","text":"Setting up a Stratum 1 \u00b6 Setting up a Stratum 1 involves the following steps: set up the Stratum 1, preferably by running the Ansible playbook that we provide; request a Stratum 0 firewall exception for your Stratum 1 server; request a cvmfs-s1-<your site>.eessi-hpc.org DNS entry; open a pull request to include the URL to your Stratum 1 in the EESSI configuration. The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site. Requirements for a Stratum 1 \u00b6 The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended. In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice. Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8. Finally, make sure that ports 80 (for the Apache web server) and 8000 are open. Step 1: set up the Stratum 1 \u00b6 The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook stratum1.yml from the filesystem-layer repository on GitHub . Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup. You can put your license key in the local configuration file inventory/local_site_specific_vars.yml . Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at templates/eessi_stratum1_squid.conf.j2 . If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file and point to it by setting local_stratum1_cvmfs_squid_conf_src in inventory/local_site_specific_vars.yml . See the comments in the example file for more details. Make sure that you have added the hostname or IP address of your server to the hosts file, and finally install the Stratum 1 using: # -b to run as root, optionally use -K if a sudo password is required ansible-playbook -b [ -K ] -e @inventory/local_site_specific_vars.yml stratum1.yml This will automatically make replicas of all the repositories defined in group_vars/all.yml . Step 2: request a firewall exception \u00b6 You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository . Make sure to include the IP address of your server. Step 3: request an EESSI DNS name \u00b6 In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name cvmfs-s1-<site>.eessi-hpc.org , where <site> is often a short name or abbreviation (e.g. rug or bgo ). You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue. Step 4: include your Stratum 1 in the EESSI configuration \u00b6 If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the urls list of the eessi_cvmfs_server_urls variable in the all.yml file .","title":"Setting up a Stratum 1"},{"location":"filesystem_layer/stratum1/#setting-up-a-stratum-1","text":"Setting up a Stratum 1 involves the following steps: set up the Stratum 1, preferably by running the Ansible playbook that we provide; request a Stratum 0 firewall exception for your Stratum 1 server; request a cvmfs-s1-<your site>.eessi-hpc.org DNS entry; open a pull request to include the URL to your Stratum 1 in the EESSI configuration. The last two steps can be skipped if you want to host a \"private\" Stratum 1 for your site.","title":"Setting up a Stratum 1"},{"location":"filesystem_layer/stratum1/#requirements-for-a-stratum-1","text":"The main requirements for a Stratum 1 server are a good network connection to the clients it is going to serve, and sufficient disk space. For the EESSI pilot, a few hundred gigabytes should suffice, but for production environments at least 1 TB would be recommended. In terms of cores and memory, a machine with just a few (~4) cores and 4-8 GB of memory should suffice. Various Linux distributions are supported, but we recommend one based on RHEL 7 or 8. Finally, make sure that ports 80 (for the Apache web server) and 8000 are open.","title":"Requirements for a Stratum 1"},{"location":"filesystem_layer/stratum1/#step-1-set-up-the-stratum-1","text":"The recommended way for setting up an EESSI Stratum 1 is by running the Ansible playbook stratum1.yml from the filesystem-layer repository on GitHub . Installing a Stratum 1 requires a GEO API license key, which will be used to find the (geographically) closest Stratum 1 server for your client and proxies. More information on how to (freely) obtain this key is available in the CVMFS documentation: https://cvmfs.readthedocs.io/en/stable/cpt-replica.html#geo-api-setup. You can put your license key in the local configuration file inventory/local_site_specific_vars.yml . Furthermore, the Stratum 1 runs a Squid server. The template configuration file can be found at templates/eessi_stratum1_squid.conf.j2 . If you want to customize it, for instance for limiting the access to the Stratum 1, you can make your own version of this template file and point to it by setting local_stratum1_cvmfs_squid_conf_src in inventory/local_site_specific_vars.yml . See the comments in the example file for more details. Make sure that you have added the hostname or IP address of your server to the hosts file, and finally install the Stratum 1 using: # -b to run as root, optionally use -K if a sudo password is required ansible-playbook -b [ -K ] -e @inventory/local_site_specific_vars.yml stratum1.yml This will automatically make replicas of all the repositories defined in group_vars/all.yml .","title":"Step 1: set up the Stratum 1"},{"location":"filesystem_layer/stratum1/#step-2-request-a-firewall-exception","text":"You can request a firewall exception rule to be added for your Stratum 1 server by opening an issue on the GitHub page of the filesystem layer repository . Make sure to include the IP address of your server.","title":"Step 2: request a firewall exception"},{"location":"filesystem_layer/stratum1/#step-3-request-an-eessi-dns-name","text":"In order to keep the configuration clean and easy, all the EESSI Stratum 1 servers have a DNS name cvmfs-s1-<site>.eessi-hpc.org , where <site> is often a short name or abbreviation (e.g. rug or bgo ). You can request this for your Stratum 1 by mentioning this in the issue that you created in Step 2, or by opening another issue.","title":"Step 3: request an EESSI DNS name"},{"location":"filesystem_layer/stratum1/#step-4-include-your-stratum-1-in-the-eessi-configuration","text":"If you want to include your Stratum 1 in the EESSI configuration, i.e. allow any (nearby) client to be able to use it, you can open a pull request with updated configuration files. You will only have to add the URL to your Stratum 1 to the urls list of the eessi_cvmfs_server_urls variable in the all.yml file .","title":"Step 4: include your Stratum 1 in the EESSI configuration"}]}